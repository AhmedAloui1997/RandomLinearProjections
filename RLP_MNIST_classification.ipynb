{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RLP vs Cross Entropy MNIST"
      ],
      "metadata": {
        "id": "s5fLmhNU-1bL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbeJbSNy-n_J"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device == \"cuda\":\n",
        "    print(\"Running on GPU\")\n",
        "else:\n",
        "    print(\"Running on CPU\")\n",
        "\n",
        "\n",
        "# Step 2: Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(2),  # Pad images to achieve 32x32 size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='./data', train=True, transform=transform, download=False)\n",
        "test_dataset = MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "## Take the first 5000 data points\n",
        "train_dataset = Subset(train_dataset, indices=range(100))\n",
        "test_dataset = Subset(test_dataset, indices=range(1000))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = x.view(-1, 16*5*5)  # Flatten\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Updated LeNet5_RLP model with softmax\n",
        "class LeNet5_RLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5_RLP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = x.view(-1, 16*5*5)  # Flatten\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.softmax(x, dim=1)  # Using softmax activation\n",
        "\n",
        "def balanced_batch_generator_mnist(data, labels, M, K):\n",
        "    num_samples = len(data)\n",
        "    selected_batches = set()\n",
        "    all_indices = np.arange(num_samples)\n",
        "\n",
        "    # Make sure each data point is in at least one batch\n",
        "    np.random.shuffle(all_indices)\n",
        "\n",
        "    # Sample until we obtain K unique batches\n",
        "    while len(selected_batches) < K:\n",
        "        # Generate indices and shuffle them\n",
        "        all_indices = np.arange(num_samples)\n",
        "        np.random.shuffle(all_indices)\n",
        "\n",
        "        # Iterate over data and form batches of size M\n",
        "        for i in range(0, num_samples, M):\n",
        "            batch_indices = tuple(sorted(all_indices[i:i+M]))\n",
        "            if batch_indices not in selected_batches:\n",
        "                if i + M > num_samples:  # skip batches smaller than N\n",
        "                  continue\n",
        "                selected_batches.add(batch_indices)\n",
        "\n",
        "            if len(selected_batches) >= K:\n",
        "                break\n",
        "\n",
        "    # Transform the set to a list\n",
        "    selected_batches = list(selected_batches)\n",
        "\n",
        "    # Yield data batches with their labels\n",
        "    for indices in selected_batches:\n",
        "        yield data[np.array(indices)], labels[np.array(indices)]\n",
        "\n",
        "iterations = 5\n",
        "num_epochs = 50\n",
        "batch_size = 95 # using batch size of 100 as given for MNIST\n",
        "num_batches = 1000\n",
        "\n",
        "# Lists to hold accuracy and recall values for both loss types\n",
        "accuracy_bce_array = np.zeros((num_epochs, iterations))\n",
        "recall_bce_array = np.zeros((num_epochs, iterations))\n",
        "accuracy_rlp_array = np.zeros((num_epochs, iterations))\n",
        "recall_rlp_array = np.zeros((num_epochs, iterations))\n",
        "\n",
        "# Extract data and labels from DataLoader\n",
        "X_train_list, y_train_list = [], []\n",
        "for data, labels in train_loader:\n",
        "    X_train_list.append(data)\n",
        "    y_train_list.append(labels)\n",
        "X_train = torch.cat(X_train_list, dim=0)\n",
        "y_train = torch.cat(y_train_list, dim=0)\n",
        "\n",
        "X_test_list, y_test_list = [], []\n",
        "for data, labels in test_loader:\n",
        "    X_test_list.append(data)\n",
        "    y_test_list.append(labels)\n",
        "X_test = torch.cat(X_test_list, dim=0)\n",
        "y_test = torch.cat(y_test_list, dim=0)\n",
        "\n",
        "#y_train_rlp = y_train.float().unsqueeze(1)  # Convert to float and add an extra dimension\n",
        "#y_test_rlp = y_test.float().unsqueeze(1)\n",
        "y_train_rlp = F.one_hot(y_train.long(), num_classes=10).float().to(device)\n",
        "y_test_rlp = F.one_hot(y_test.long(), num_classes=10).float().to(device)\n",
        "\n",
        "unique_batches = list(balanced_batch_generator_mnist(X_train, y_train_rlp, batch_size, num_batches))\n",
        "print(\"All the unique batches have been generated\")\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "\n",
        "for i in range(iterations):\n",
        "    print(\"This is iterations number: \",i)\n",
        "    # Reset models\n",
        "    model_bce = LeNet5()\n",
        "    model_rlp = LeNet5_RLP()\n",
        "    model_bce = model_bce.to(device)\n",
        "    model_rlp = model_rlp.to(device)\n",
        "\n",
        "    # Split data into train and test\n",
        "\n",
        "    optimizer_bce = optim.AdamW(model_bce.parameters(), lr=2e-3)\n",
        "    optimizer_rlp = optim.AdamW(model_rlp.parameters(), lr=2e-3)\n",
        "    criterion_bce = nn.CrossEntropyLoss()\n",
        "    criterion_rlp = nn.MSELoss()\n",
        "\n",
        "    # Convert labels for BCE\n",
        "    y_train_bce = y_train.long()\n",
        "    y_test_bce = y_test.long()\n",
        "    y_train_bce = y_train_bce.to(device)\n",
        "    y_train_rlp = y_train_rlp.to(device)\n",
        "\n",
        "    # Convert labels for RLP\n",
        "    #y_train_rlp = y_train.float().unsqueeze(1)\n",
        "    #y_test_rlp = y_test.float().unsqueeze(1)\n",
        "    y_train_rlp = F.one_hot(y_train.long(), num_classes=10).float().to(device)\n",
        "    y_test_rlp = y_test.long() #F.one_hot(y_test.long(), num_classes=10).float().to(device)\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop for BCE\n",
        "        optimizer_bce.zero_grad()\n",
        "        outputs_bce = model_bce(X_train)\n",
        "        loss_bce = criterion_bce(outputs_bce, y_train_bce)\n",
        "        loss_bce.backward()\n",
        "        optimizer_bce.step()\n",
        "\n",
        "        # Training loop for RLP\n",
        "        for batch_X, batch_y in unique_batches:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            model_rlp.train()\n",
        "            optimizer_rlp.zero_grad()\n",
        "            outputs_rlp = model_rlp(batch_X)\n",
        "\n",
        "            # Reshape batch_X for matrix operations\n",
        "            batch_X_reshaped = batch_X.view(batch_X.size(0), -1)\n",
        "\n",
        "            # Compute pseudo-inverse\n",
        "            reg_matrix = torch.linalg.pinv(batch_X_reshaped.transpose(0, 1) @ batch_X_reshaped) @ batch_X_reshaped.transpose(0, 1)\n",
        "            # Matrix multiplications\n",
        "            c = reg_matrix @ batch_y\n",
        "            c_pred = reg_matrix @ outputs_rlp\n",
        "\n",
        "            loss_rlp = criterion_rlp(batch_X_reshaped @ c_pred , batch_X_reshaped @ c )\n",
        "            loss_rlp.backward()\n",
        "            optimizer_rlp.step()\n",
        "\n",
        "        X_test = X_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "        y_test_bce = y_test_bce.to(device)\n",
        "        y_test_rlp = y_test_rlp.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_bce_test = model_bce(X_test)\n",
        "            _, predicted_bce = torch.max(outputs_bce_test, 1)\n",
        "            accuracy_bce_array[epoch,i] = (predicted_bce == y_test_bce).sum().item() / len(y_test) * 100\n",
        "            recall_bce_array[epoch,i] = f1_score(y_test_bce.cpu().numpy(), predicted_bce.cpu().numpy(), average='macro')\n",
        "\n",
        "\n",
        "        # Evaluation for RLP\n",
        "        with torch.no_grad():\n",
        "            outputs_rlp_test = model_rlp(X_test)\n",
        "            _, predicted_rlp = torch.max(outputs_rlp_test, 1)\n",
        "            accuracy_rlp_array[epoch, i] = (predicted_rlp == y_test_rlp).sum().item() / len(y_test) * 100\n",
        "            recall_rlp_array[epoch,i] = f1_score(y_test_rlp.cpu().numpy(), predicted_rlp.cpu().numpy(), average='macro')\n",
        "\n",
        "\n",
        "        print(f'Epoch: {epoch}, BCE Accuracy: {accuracy_bce_array[epoch,i]}, BCE Recall: {recall_bce_array[epoch,i]}')\n",
        "        print(f'Epoch: {epoch}, RLP Accuracy: {accuracy_rlp_array[epoch,i]}, RLP Recall: {recall_rlp_array[epoch,i]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RLP vs Cross Entropy on top of MIXUP data augmentation"
      ],
      "metadata": {
        "id": "EXgU4q9_-5he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device == \"cuda\":\n",
        "    print(\"Running on GPU\")\n",
        "else:\n",
        "    print(\"Running on CPU\")\n",
        "\n",
        "\n",
        "# Step 2: Data Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(2),  # Pad images to achieve 32x32 size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='./data', train=True, transform=transform, download=False)\n",
        "test_dataset = MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "## Take the first 5000 data points\n",
        "train_dataset = Subset(train_dataset, indices=range(5000))\n",
        "test_dataset = Subset(test_dataset, indices=range(5000))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = x.view(-1, 16*5*5)  # Flatten\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Updated LeNet5_RLP model with softmax\n",
        "class LeNet5_RLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5_RLP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "        x = F.avg_pool2d(x, 2, stride=2)\n",
        "        x = x.view(-1, 16*5*5)  # Flatten\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.softmax(x, dim=1)  # Using softmax activation\n",
        "\n",
        "def balanced_batch_generator_mnist(data, labels, M, K):\n",
        "    num_samples = len(data)\n",
        "    selected_batches = set()\n",
        "    all_indices = np.arange(num_samples)\n",
        "\n",
        "    # Make sure each data point is in at least one batch\n",
        "    np.random.shuffle(all_indices)\n",
        "\n",
        "    # Sample until we obtain K unique batches\n",
        "    while len(selected_batches) < K:\n",
        "        # Generate indices and shuffle them\n",
        "        all_indices = np.arange(num_samples)\n",
        "        np.random.shuffle(all_indices)\n",
        "\n",
        "        # Iterate over data and form batches of size M\n",
        "        for i in range(0, num_samples, M):\n",
        "            batch_indices = tuple(sorted(all_indices[i:i+M]))\n",
        "            if batch_indices not in selected_batches:\n",
        "                if i + M > num_samples:  # skip batches smaller than N\n",
        "                  continue\n",
        "                selected_batches.add(batch_indices)\n",
        "\n",
        "            if len(selected_batches) >= K:\n",
        "                break\n",
        "\n",
        "    # Transform the set to a list\n",
        "    selected_batches = list(selected_batches)\n",
        "\n",
        "    # Yield data batches with their labels\n",
        "    for indices in selected_batches:\n",
        "        yield data[np.array(indices)], labels[np.array(indices)]\n",
        "\n",
        "iterations = 5\n",
        "num_epochs = 50\n",
        "batch_size = 28*28+1 # using batch size of 100 as given for MNIST\n",
        "num_batches = 1000\n",
        "\n",
        "# Lists to hold accuracy and recall values for both loss types\n",
        "accuracy_bce_array = np.zeros((num_epochs, iterations))\n",
        "recall_bce_array = np.zeros((num_epochs, iterations))\n",
        "accuracy_rlp_array = np.zeros((num_epochs, iterations))\n",
        "recall_rlp_array = np.zeros((num_epochs, iterations))\n",
        "\n",
        "# Extract data and labels from DataLoader\n",
        "X_train_list, y_train_list = [], []\n",
        "for data, labels in train_loader:\n",
        "    X_train_list.append(data)\n",
        "    y_train_list.append(labels)\n",
        "X_train = torch.cat(X_train_list, dim=0)\n",
        "y_train = torch.cat(y_train_list, dim=0)\n",
        "\n",
        "X_test_list, y_test_list = [], []\n",
        "for data, labels in test_loader:\n",
        "    X_test_list.append(data)\n",
        "    y_test_list.append(labels)\n",
        "X_test = torch.cat(X_test_list, dim=0)\n",
        "y_test = torch.cat(y_test_list, dim=0)\n",
        "\n",
        "#y_train_rlp = y_train.float().unsqueeze(1)  # Convert to float and add an extra dimension\n",
        "#y_test_rlp = y_test.float().unsqueeze(1)\n",
        "y_train_rlp = F.one_hot(y_train.long(), num_classes=10).float().to(device)\n",
        "y_test_rlp = F.one_hot(y_test.long(), num_classes=10).float().to(device)\n",
        "\n",
        "unique_batches = list(balanced_batch_generator_mnist(X_train, y_train_rlp, batch_size, num_batches))\n",
        "np.random.shuffle(unique_batches)  # Shuffle the batches for the second iterator\n",
        "unique_batches2 = list(unique_batches)\n",
        "print(\"All the unique batches have been generated\")\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "\n",
        "for i in range(iterations):\n",
        "    print(\"This is iterations number: \",i)\n",
        "    # Reset models\n",
        "    model_bce = LeNet5()\n",
        "    model_rlp = LeNet5_RLP()\n",
        "    model_bce = model_bce.to(device)\n",
        "    model_rlp = model_rlp.to(device)\n",
        "\n",
        "    # Split data into train and test\n",
        "\n",
        "    optimizer_bce = optim.AdamW(model_bce.parameters(), lr=2e-3)\n",
        "    optimizer_rlp = optim.AdamW(model_rlp.parameters(), lr=2e-3)\n",
        "    criterion_bce = nn.CrossEntropyLoss()\n",
        "    criterion_rlp = nn.MSELoss()\n",
        "\n",
        "    # Convert labels for BCE\n",
        "    y_train_bce = y_train.long()\n",
        "    y_test_bce = y_test.long()\n",
        "    y_train_bce = y_train_bce.to(device)\n",
        "    y_train_rlp = y_train_rlp.to(device)\n",
        "\n",
        "    # Convert labels for RLP\n",
        "    #y_train_rlp = y_train.float().unsqueeze(1)\n",
        "    #y_test_rlp = y_test.float().unsqueeze(1)\n",
        "    y_train_rlp = F.one_hot(y_train.long(), num_classes=10).float().to(device)\n",
        "    y_test_rlp = y_test.long() #F.one_hot(y_test.long(), num_classes=10).float().to(device)\n",
        "\n",
        "    #x_train and y_trainn loading\n",
        "    batch_size_bce = 500\n",
        "    train_dataset = TensorDataset(X_train, y_train_bce)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size_bce, shuffle=True)\n",
        "    train_dataloader_2 = DataLoader(train_dataset, batch_size_bce, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop for BCE\n",
        "        for (x1, y1), (x2, y2) in zip(train_dataloader, train_dataloader_2):\n",
        "            model_bce.train()\n",
        "            alpha = 0.15\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "            x = lam * x1 + (1. - lam) * x2\n",
        "            y = lam * y1 + (1. - lam) * y2\n",
        "            y = torch.LongTensor(y.cpu().numpy())\n",
        "            y = y.to(device)\n",
        "            x = x.to(device)\n",
        "\n",
        "            optimizer_bce.zero_grad()\n",
        "            outputs = model_bce(x)\n",
        "            loss_bce = criterion_bce(outputs, y)\n",
        "            loss_bce.backward()\n",
        "            optimizer_bce.step()\n",
        "\n",
        "        #optimizer_bce.zero_grad()\n",
        "        #outputs_bce = model_bce(X_train)\n",
        "        #loss_bce = criterion_bce(outputs_bce, y_train_bce)\n",
        "\n",
        "\n",
        "        # Training loop for RLP\n",
        "        for (batch_X, batch_y), (batch_X2, batch_y2) in zip(unique_batches, unique_batches2):\n",
        "            model_rlp.train()\n",
        "            alpha = 0.15\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "            x = lam * batch_X + (1. - lam) * batch_X2\n",
        "            y = lam * batch_y + (1. - lam) * batch_y2\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer_rlp.zero_grad()\n",
        "            outputs_rlp = model_rlp(x)\n",
        "\n",
        "            # Reshape batch_X for matrix operations\n",
        "            x = x.view(batch_X.size(0), -1)\n",
        "\n",
        "            # Compute pseudo-inverse\n",
        "            reg_matrix = torch.linalg.pinv(x.transpose(0, 1) @ x) @ x.transpose(0, 1)\n",
        "            # Matrix multiplications\n",
        "            c = reg_matrix @ y\n",
        "            c_pred = reg_matrix @ outputs_rlp\n",
        "\n",
        "            loss_rlp = criterion_rlp(x @ c_pred , x @ c )\n",
        "            loss_rlp.backward()\n",
        "            optimizer_rlp.step()\n",
        "\n",
        "        X_test = X_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "        y_test_bce = y_test_bce.to(device)\n",
        "        y_test_rlp = y_test_rlp.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_bce_test = model_bce(X_test)\n",
        "            _, predicted_bce = torch.max(outputs_bce_test, 1)\n",
        "            accuracy_bce_array[epoch,i] = (predicted_bce == y_test_bce).sum().item() / len(y_test) * 100\n",
        "            recall_bce_array[epoch,i] = f1_score(y_test_bce.cpu().numpy(), predicted_bce.cpu().numpy(), average='macro')\n",
        "\n",
        "\n",
        "        # Evaluation for RLP\n",
        "        with torch.no_grad():\n",
        "            outputs_rlp_test = model_rlp(X_test)\n",
        "            _, predicted_rlp = torch.max(outputs_rlp_test, 1)\n",
        "            accuracy_rlp_array[epoch, i] = (predicted_rlp == y_test_rlp).sum().item() / len(y_test) * 100\n",
        "            recall_rlp_array[epoch,i] = f1_score(y_test_rlp.cpu().numpy(), predicted_rlp.cpu().numpy(), average='macro')\n",
        "\n",
        "\n",
        "        print(f'Epoch: {epoch}, BCE Accuracy: {accuracy_bce_array[epoch,i]}, BCE Recall: {recall_bce_array[epoch,i]}')\n",
        "        print(f'Epoch: {epoch}, RLP Accuracy: {accuracy_rlp_array[epoch,i]}, RLP Recall: {recall_rlp_array[epoch,i]}')\n",
        "\n"
      ],
      "metadata": {
        "id": "bWJDg6lg--tp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}